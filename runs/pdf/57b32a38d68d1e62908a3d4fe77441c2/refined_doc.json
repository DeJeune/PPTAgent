{"metadata": {"title": "Building Effective Agents", "author": "Erik Schluntz and Barry Zhang", "publish date": "2024\u5e7412\u670820\u65e5", "organization": "Anthropic"}, "sections": [{"title": "What are agents?", "subsections": [{"title": "Definition and Variations", "content": "An 'agent' can be defined in several ways. Some customers define agents as fully autonomous systems that operate independently over extended periods, using various tools to accomplish complex tasks. Others use the term to describe more prescriptive implementations that follow predefined workflows. At Anthropic, we categorize all these variations as agentic systems, but draw an important architectural distinction between workflows and agents: Workflows are systems where LLMs and tools are orchestrated through predefined code paths. Agents, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks."}]}, {"title": "When (and when not) to use agents", "subsections": [{"title": "General Guidance", "content": "When building applications with LLMs, we recommend finding the simplest solution possible, and only increasing complexity when needed. This might mean not building agentic systems at all. Agentic systems often trade latency and cost for better task performance, and you should consider when this tradeoff makes sense. When more complexity is warranted, workflows offer predictability and consistency for well-defined tasks, whereas agents are the better option when flexibility and model-driven decision-making are needed at scale."}]}, {"title": "When and how to use frameworks", "subsections": [{"title": "Common Frameworks", "content": "There are many frameworks that make agentic systems easier to implement, including LangGraph from LangChain, Amazon Bedrock's AI Agent framework, Rivet, and Vellum. These frameworks simplify standard low-level tasks like calling LLMs, defining and parsing tools, and chaining calls together. However, they often create extra layers of abstraction that can obscure the underlying prompts and responses, making them harder to debug. We suggest that developers start by using LLM APIs directly: many patterns can be implemented in a few lines of code. If you do use a framework, ensure you understand the underlying code."}]}, {"title": "Building blocks, workflows, and agents", "subsections": [{"title": "Building Block: The Augmented LLM", "content": "The basic building block of agentic systems is an LLM enhanced with augmentations such as retrieval, tools, and memory. Our current models can actively use these capabilities\u2014generating their own search queries, selecting appropriate tools, and determining what information to retain. We recommend focusing on two key aspects of the implementation: tailoring these capabilities to your specific use case and ensuring they provide an easy, well-documented interface for your LLM."}, {"title": "Workflow: Prompt Chaining", "content": "Prompt chaining decomposes a task into a sequence of steps, where each LLM call processes the output of the previous one. You can add programmatic checks on any intermediate steps to ensure that the process is still on track. This workflow is ideal for situations where the task can be easily and cleanly decomposed into fixed subtasks. The main goal is to trade off latency for higher accuracy, by making each LLM call an easier task."}, {"title": "Workflow: Routing", "content": "Routing classifies an input and directs it to a specialized follow-up task. This workflow allows for separation of concerns, and building more specialized prompts. Without this workflow, optimizing for one kind of input can hurt performance on other inputs. Routing works well for complex tasks where there are distinct categories that are better handled separately, and where classification can be handled accurately, either by an LLM or a more traditional classification model/algorithm."}, {"title": "Workflow: Parallelization", "content": "LLMs can sometimes work simultaneously on a task and have their outputs aggregated programmatically. This workflow, parallelization, manifests in two key variations: Sectioning, which breaks a task into independent subtasks run in parallel, and Voting, which runs the same task multiple times to get diverse outputs. Parallelization is effective when the divided subtasks can be parallelized for speed, or when multiple perspectives or attempts are needed for higher confidence results."}, {"title": "Workflow: Orchestrator-workers", "content": "In the orchestrator-workers workflow, a central LLM dynamically breaks down tasks, delegates them to worker LLMs, and synthesizes their results. This workflow is particularly useful for complex tasks where you can't predict the subtasks needed, such as in coding tasks where the number of files that need to be changed and the nature of the change in each file depend on the task. The key difference from parallelization is its flexibility\u2014subtasks aren't predefined, but determined by the orchestrator based on the specific input."}, {"title": "Workflow: Evaluator-optimizer", "content": "In the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop. This workflow is particularly effective when we have clear evaluation criteria, and when iterative refinement provides measurable value. The two signs of good fit are, first, that LLM responses can be demonstrably improved when a human articulates their feedback; and second, that the LLM can provide such feedback. This is analogous to the iterative writing process a human writer might go through when producing a polished document."}]}, {"title": "Agents", "subsections": [{"title": "Autonomous Agents", "content": "Agents are emerging in production as LLMs mature in key capabilities\u2014understanding complex inputs, engaging in reasoning and planning, using tools reliably, and recovering from errors. Agents begin their work with a command from, or interactive discussion with, the human user. Once the task is clear, agents plan and operate independently, potentially returning to the human for further information or judgment. During execution, it's crucial for the agents to gain 'ground truth' from the environment at each step to assess its progress. Agents can then pause for human feedback at checkpoints or when encountering blockers. The task often terminates upon completion, but it's also common to include stopping conditions to maintain control."}, {"title": "When to Use Agents", "content": "Agents can be used for open-ended problems where it's difficult or impossible to predict the required number of steps, and where you can't hardcode a fixed path. The LLM will potentially operate for many turns, and you must have some level of trust in its decision-making. Agents' autonomy makes them ideal for scaling tasks in trusted environments. However, the autonomous nature of agents means higher costs and the potential for compounding errors. We recommend extensive testing in sandboxed environments, along with the appropriate guardrails."}]}, {"title": "Combining and Customizing These Patterns", "subsections": [{"title": "Key to Success", "content": "These building blocks aren't prescriptive. They're common patterns that developers can shape and combine to fit different use cases. The key to success, as with any LLM features, is measuring performance and iterating on implementations. You should consider adding complexity only when it demonstrably improves outcomes."}]}, {"title": "Summary", "subsections": [{"title": "Key Principles", "content": "Success in the LLM space isn't about building the most sophisticated system. It's about building the right system for your needs. Start with simple prompts, optimize them with comprehensive evaluation, and add multi-step agentic systems only when simpler solutions fall short. When implementing agents, we try to follow three core principles: 1. Maintain simplicity in your agent's design. 2. Prioritize transparency by explicitly showing the agent's planning steps. 3. Carefully craft your agent-computer interface (ACI) through thorough tool documentation and testing."}]}, {"title": "Appendix 1: Agents in Practice", "subsections": [{"title": "A. Customer Support", "content": "Customer support combines familiar chatbot interfaces with enhanced capabilities through tool integration. This is a natural fit for more open-ended agents because support interactions naturally follow a conversation flow while requiring access to external information and actions. Tools can be integrated to pull customer data, order history, and knowledge base articles. Actions such as issuing refunds or updating tickets can be handled programmatically. Success can be clearly measured through user-defined resolutions. Several companies have demonstrated the viability of this approach through usage-based pricing models that charge only for successful resolutions, showing confidence in their agents' effectiveness."}, {"title": "B. Coding Agents", "content": "The software development space has shown remarkable potential for LLM features, with capabilities evolving from code completion to autonomous problem-solving. Agents are particularly effective because code solutions are verifiable through automated tests, agents can iterate on solutions using test results as feedback, the problem space is well-defined and structured, and output quality can be measured objectively. In our own implementation, agents can now solve real GitHub issues in the SWEbench Verified benchmark based on the pull request description alone. However, while automated testing helps verify functionality, human review remains crucial for ensuring solutions align with broader system requirements."}]}, {"title": "Appendix 2: Prompt Engineering Your Tools", "subsections": [{"title": "Best Practices", "content": "No matter which agentic system you're building, tools will likely be an important part of your agent. Tools enable Claude to interact with external services and APIs by specifying their exact structure and definition in our API. Tool definitions and specifications should be given just as much prompt engineering attention as your overall prompts. When deciding on tool formats, give the model enough tokens to 'think' before it writes itself into a corner. Keep the format close to what the model has seen naturally occurring in text on the internet. Make sure there's no formatting 'overhead' such as having to keep an accurate count of thousands of lines of code, or string-escaping any code it writes. Put yourself in the model's shoes. Is it obvious how to use this tool, based on the description and parameters, or would you need to think carefully about it? If so, then it's probably also true for the model. Test how the model uses your tools: Run many example inputs in our workbench to see what mistakes the model makes, and iterate. While building our agent for SWE-bench, we actually spent more time optimizing our tools than the overall prompt. For example, we found that the model would make mistakes with tools using relative filepaths after the agent had moved out of the root directory. To fix this, we changed the tool to always require absolute filepaths\u2014and we found that the model used this method flawlessly."}]}]}